# trainning args : 

fea_cfg         =       #   default_value       necessary 1 helper: feature_config_path
train           =       #   default_value       necessary 0 helper: trainning data path, use stdin(standard input) if not set
valid           =       #   default_value       necessary 0 helper: validation data path
valid_interval  =  60   #   default_value 60    necessary 0 helper: how many seconds between two validition
dim             =  8    #   default_value 8     necessary 0 helper: feature vector dim(factor_num)
im              =       #   default_value       necessary 0 helper: init model path
om              =       #   default_value       necessary 0 helper: output model path
mf              =  txt  #   default_value txt   necessary 0 helper: output model format
threads         =  11   #   default_value 11    necessary 0 helper: trainning threads_num
fea_sep         =       #   default_value 	    necessary 0 helper: specify one character (or str "blank","tab") for fea_seperator in line of sample. 
fea_kv_sep      =  :    #   default_value :     necessary 0 helper: specify one character (or str "blank","tab", "eqaul"(for "="))  for fea_kv_seperator in line of sample. 
fea_values_sep  =  ,    #   default_value ,     necessary 0 helper: specify one character(or str "blank","tab")  for fea_multivalue_seperator in line of sample. 

# feature id mapping settings

disable_feaid_mapping     # option. helper: ignore feature ID mapping dicts, using hash or original ID.
id_map_dict_sep        =  #default_value       necessary 0 helper: specify one character(or str "blank","tab") for k_v_seperator in line of feature mapping dict

verbose    =   1       #   default_value 1       necessary 0 helper: 0: only trainning logs. 1: open feature config messages, 2 : open debug messages

#h                     #   option. helper: print help message
epoch      =   1      # train epochs

# loss (for all learners): 
l1w   =   0.05    #   default_value 0.05    necessary 0  helper: l1 regularization of w for all learners
l2w   =   5       #   default_value 5       necessary 0  helper: l2 regularization of w for all learners
l1v   =   0.05    #   default_value 0.05    necessary 0  helper: l1 regularization of V for all learners
l2v   =   5       #   default_value 5       necessary 0  helper: l2 regularization of V for all learners

# sgdm hyper-paramerters : (SGD with Momentum)
sgdm.lr   =   0.001   #   default_value 0.001   necessary 0 helper: SGD hyper-param
sgdm.beta1       =   0.9      #   default_value 0.9     

# adam hyper-paramerters : 
adam.lr       =  0.0001  # defaut: 0.001  Adam learning rate
adam.beta1           =  0.9    # defaut: 0.9    adam一阶动量平滑常数
adam.bias_correct    =  1      # defaut: 1      bias_correct
adam.beta2           =  0.999  # defaut: 0.999  adam二阶动量平滑常数
adam.weight_decay_w  =  0.0001 # defaut: 0.0001 l2正则在adam中的实现。对于adam，宜用weight_decay，不宜用l2正则
adam.weight_decay_V  =  0.0001 # defaut: 0.0001 l2正则在adam中的实现。对于adam，宜用weight_decay，不宜用l2正则
adam.amsgrad         =  0      # defaut: 0      保留历史最大的v_t，记为v_{max}，每次计算都是用最大的v_{max}，否则是用当前v_t

# FTRL hyper-paramerters : 
ftrl.init_stdev =   0.001   #   default_value 0.001   necessary 0 helper: stdev for initialization of 2-way factors
ftrl.w_alpha    =   0.01    #   default_value 0.01    necessary 0 helper: FTRL hyper-param
ftrl.w_beta     =   1       #   default_value 1       necessary 0 helper: FTRL hyper-param
ftrl.v_alpha    =   0.01    #   default_value 0.01    necessary 0 helper: FTRL hyper-param
ftrl.v_beta     =   1       #   default_value 1       necessary 0 helper: FTRL hyper-param
