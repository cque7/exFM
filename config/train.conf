# trainning args : 

fea_cfg         =       #   default_value       necessary 1 helper: feature_config_path
train           =       #   default_value       necessary 0 helper: trainning data path, use stdin(standard input) if not set
valid           =       #   default_value       necessary 0 helper: validation data path
valid_interval  =  60   #   default_value 60    necessary 0 helper: how many seconds between two validition
im              =       #   default_value       necessary 0 helper: init model path
om              =       #   default_value       necessary 0 helper: output model path
mf              =  txt  #   default_value txt   necessary 0 helper: output model format
threads         =  11   #   default_value 11    necessary 0 helper: trainning threads_num
fea_sep         =       #   default_value 	    necessary 0 helper: specify one character (or str "blank","tab") for fea_seperator in line of sample. 
fea_kv_sep      =  :    #   default_value :     necessary 0 helper: specify one character (or str "blank","tab", "eqaul"(for "="))  for fea_kv_seperator in line of sample. 
fea_values_sep  =  ,    #   default_value ,     necessary 0 helper: specify one character(or str "blank","tab")  for fea_multivalue_seperator in line of sample. 

# feature id mapping settings

id_map_dict_sep        =  #default_value       necessary 0 helper: specify one character(or str "blank","tab") for k_v_seperator in line of feature mapping dict

verbose    =   1       #   default_value 1       necessary 0 helper: 0: only trainning logs. 1: open feature config messages, 2 : open debug messages

#h                     #   option. helper: print help message
epoch      =   7       # train epochs


batch_size      = 1024 

# sgdm(SGD with Momentum) hyper-paramerters : (activated when solver=sgdm) :
sgdm.lr         =   0.001   #   default_value 0.001   necessary 0 helper: SGD hyper-param
sgdm.beta1      =   0.9     #   default_value 0.9     
sgdm.l1w        =   0.05    #   default_value 0.05    necessary 0  helper: l1 regularization of w
sgdm.l2w        =   1       #   default_value 5       necessary 0  helper: l2 regularization of w
sgdm.l1v        =   0.05    #   default_value 0.05    necessary 0  helper: l1 regularization of V
sgdm.l2v        =   1       #   default_value 5       necessary 0  helper: l2 regularization of V

# adagrad hyper-paramerters (activated when solver=adagrad) :
adagrad.lr = 0.01
adagrad.l2_norm_w = 1e-5
adagrad.l2_norm_V = 1e-5

# adam hyper-paramerters (activated when solver=adam) :
adam.lr              =  0.0001   # defaut: 0.001  Adam learning rate
adam.beta1           =  0.9      # defaut: 0.9    adam一阶动量平滑常数
adam.beta2           =  0.999    # defaut: 0.999  adam二阶动量平滑常数
adam.weight_decay_w  =  0.1      # defaut: 2        l2正则在adam中的实现。对于adam，宜用weight_decay，不宜用l2正则。Adam通常需要比SGD更多的regularization
adam.weight_decay_V  =  0.1      # defaut: 2        同上

# FTRL hyper-paramerters (activated when solver=ftrl) :
init_stdev =   0.001   #   default_value 0.001   necessary 0 helper: stdev for initialization of 2-way factors
ftrl.w_alpha    =   0.01    #   default_value 0.01    necessary 0 helper: FTRL hyper-param
ftrl.v_alpha    =   0.01    #   default_value 0.01    necessary 0 helper: FTRL hyper-param
ftrl.w_beta     =   1       #   default_value 1       necessary 0 helper: FTRL hyper-param
ftrl.v_beta     =   1       #   default_value 1       necessary 0 helper: FTRL hyper-param
ftrl.l1w        =   0.05    #   default_value 0.05    necessary 0  helper: l1 regularization of w  
ftrl.l2w        =   5       #   default_value 5       necessary 0  helper: l2 regularization of w 
ftrl.l1v        =   0.05    #   default_value 0.05    necessary 0  helper: l1 regularization of V 
ftrl.l2v        =   5       #   default_value 5       necessary 0  helper: l2 regularization of V 
